{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "  Model   RMSE Hit Rate Precision@10 MAP@10 MRR@10 Coverage\n",
      "    KNN 0.9789   0.0232       0.0025 0.0016 0.0064   0.4753\n",
      "    SVD 0.9333   0.0544       0.0057 0.0039 0.0154   0.1760\n",
      "TOP_POP 3.3354   0.1125       0.0117 0.0145 0.0625   0.0161\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Results saved to 'week8_results.csv'\n"
     ]
    }
   ],
   "source": [
    "# STEP 6: Display Results\n",
    "\n",
    "\n",
    "# Create results table\n",
    "results = []\n",
    "\n",
    "for model_name in ['knn', 'svd', 'top_pop']:\n",
    "    rmse_val = {\n",
    "        'knn': knn_rmse,\n",
    "        'svd': svd_rmse,\n",
    "        'top_pop': top_pop_rmse\n",
    "    }[model_name]\n",
    "    \n",
    "    row = {\n",
    "        'Model': model_name.upper(),\n",
    "        'RMSE': f\"{rmse_val:.4f}\",\n",
    "        'Hit Rate': f\"{np.mean(metrics[model_name]['hit_rate']):.4f}\" if metrics[model_name]['hit_rate'] else \"N/A\",\n",
    "        'Precision@10': f\"{np.mean(metrics[model_name]['precision']):.4f}\" if metrics[model_name]['precision'] else \"N/A\",\n",
    "        'MAP@10': f\"{np.mean(metrics[model_name]['ap']):.4f}\" if metrics[model_name]['ap'] else \"N/A\",\n",
    "        'MRR@10': f\"{np.mean(metrics[model_name]['mrr']):.4f}\" if metrics[model_name]['mrr'] else \"N/A\",\n",
    "        'Coverage': f\"{coverage[model_name]:.4f}\"\n",
    "    }\n",
    "    results.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('week8_results.csv', index=False)\n",
    "print(\"\\nResults saved to 'week8_results.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics...\n"
     ]
    }
   ],
   "source": [
    "# STEP 5: Calculate Ranking Metrics\n",
    "\n",
    "def calculate_hit_rate(recommendations, test_df, user_id, threshold=3):\n",
    "    \"\"\"Hit Rate = 1 if at least one relevant item in recommendations\"\"\"\n",
    "    user_test = test_df[test_df['user_id'] == user_id]\n",
    "    relevant_items = set(user_test[user_test['rating'] >= threshold]['item_id'])\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    hits = set(recommendations) & relevant_items\n",
    "    return 1 if len(hits) > 0 else 0\n",
    "\n",
    "def calculate_precision_at_k(recommendations, test_df, user_id, k=10, threshold=3):\n",
    "    \"\"\"Precision@k = (# relevant items in top-k) / k\"\"\"\n",
    "    user_test = test_df[test_df['user_id'] == user_id]\n",
    "    relevant_items = set(user_test[user_test['rating'] >= threshold]['item_id'])\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    relevant_in_recs = len(set(recommendations[:k]) & relevant_items)\n",
    "    return relevant_in_recs / k\n",
    "\n",
    "def calculate_ap_at_k(recommendations, test_df, user_id, k=10, threshold=3):\n",
    "    \"\"\"Average Precision@k\"\"\"\n",
    "    user_test = test_df[test_df['user_id'] == user_id]\n",
    "    relevant_items = set(user_test[user_test['rating'] >= threshold]['item_id'])\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    ap = 0\n",
    "    num_relevant = 0\n",
    "    \n",
    "    for i, item in enumerate(recommendations[:k], 1):\n",
    "        if item in relevant_items:\n",
    "            num_relevant += 1\n",
    "            ap += num_relevant / i\n",
    "    \n",
    "    return ap / min(k, len(relevant_items))\n",
    "\n",
    "def calculate_mrr_at_k(recommendations, test_df, user_id, k=10, threshold=3):\n",
    "    \"\"\"Mean Reciprocal Rank@k\"\"\"\n",
    "    user_test = test_df[test_df['user_id'] == user_id]\n",
    "    relevant_items = set(user_test[user_test['rating'] >= threshold]['item_id'])\n",
    "    \n",
    "    if len(relevant_items) == 0:\n",
    "        return None\n",
    "    \n",
    "    for i, item in enumerate(recommendations[:k], 1):\n",
    "        if item in relevant_items:\n",
    "            return 1 / i\n",
    "    \n",
    "    return 0\n",
    "\n",
    "def calculate_coverage(recommendations_dict, all_items):\n",
    "    \"\"\"Coverage = (# unique items recommended) / (total # items)\"\"\"\n",
    "    all_recommended = set()\n",
    "    for user_recs in recommendations_dict.values():\n",
    "        all_recommended.update(user_recs)\n",
    "    \n",
    "    return len(all_recommended) / len(all_items)\n",
    "\n",
    "# Calculate metrics for each model\n",
    "print(\"Calculating metrics...\")\n",
    "\n",
    "metrics = {\n",
    "    'knn': {'hit_rate': [], 'precision': [], 'ap': [], 'mrr': []},\n",
    "    'svd': {'hit_rate': [], 'precision': [], 'ap': [], 'mrr': []},\n",
    "    'top_pop': {'hit_rate': [], 'precision': [], 'ap': [], 'mrr': []}\n",
    "}\n",
    "\n",
    "for user_id in test_users:\n",
    "    for model_name in ['knn', 'svd', 'top_pop']:\n",
    "        recs = user_recommendations[user_id][model_name]\n",
    "        \n",
    "        if len(recs) == 0:\n",
    "            continue\n",
    "            \n",
    "        hit = calculate_hit_rate(recs, test_df, user_id)\n",
    "        if hit is not None:\n",
    "            metrics[model_name]['hit_rate'].append(hit)\n",
    "        \n",
    "        prec = calculate_precision_at_k(recs, test_df, user_id)\n",
    "        if prec is not None:\n",
    "            metrics[model_name]['precision'].append(prec)\n",
    "        \n",
    "        ap = calculate_ap_at_k(recs, test_df, user_id)\n",
    "        if ap is not None:\n",
    "            metrics[model_name]['ap'].append(ap)\n",
    "        \n",
    "        mrr = calculate_mrr_at_k(recs, test_df, user_id)\n",
    "        if mrr is not None:\n",
    "            metrics[model_name]['mrr'].append(mrr)\n",
    "\n",
    "# Calculate coverage\n",
    "all_items = set(train_df['item_id'].unique())\n",
    "\n",
    "coverage = {\n",
    "    'knn': calculatge_coverage({u: user_recommendations[u]['knn'] for u in test_users}, all_items),\n",
    "    'svd': calculate_coverage({u: user_recommendations[u]['svd'] for u in test_users}, all_items),\n",
    "    'top_pop': calculate_coverage({u: user_recommendations[u]['top_pop'] for u in test_users}, all_items)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating recommendations for 1389 test users...\n",
      "Processed 0/1389 users...\n",
      "Processed 100/1389 users...\n",
      "Processed 200/1389 users...\n",
      "Processed 300/1389 users...\n",
      "Processed 400/1389 users...\n",
      "Processed 500/1389 users...\n",
      "Processed 600/1389 users...\n",
      "Processed 700/1389 users...\n",
      "Processed 800/1389 users...\n",
      "Processed 900/1389 users...\n",
      "Processed 1000/1389 users...\n",
      "Processed 1100/1389 users...\n",
      "Processed 1200/1389 users...\n",
      "Processed 1300/1389 users...\n",
      "Recommendations generated for all 1389 users!\n"
     ]
    }
   ],
   "source": [
    "# STEP 4: Generate top-k recommendations for test users\n",
    "\n",
    "def get_top_n_recommendations(algo, trainset, user_id, n=10):\n",
    "    \"\"\"Get top N recommendations for a user\"\"\"\n",
    "    try:\n",
    "        # Get all items\n",
    "        all_items = set([trainset.to_raw_iid(iid) for iid in range(trainset.n_items)])\n",
    "        \n",
    "        # Get items the user has already rated\n",
    "        try:\n",
    "            user_inner_id = trainset.to_inner_uid(user_id)\n",
    "            user_rated = set([trainset.to_raw_iid(iid) for (iid, _) in trainset.ur[user_inner_id]])\n",
    "        except ValueError:\n",
    "            user_rated = set()\n",
    "        \n",
    "        # Items to recommend = all items minus rated items\n",
    "        items_to_recommend = list(all_items - user_rated)\n",
    "        \n",
    "        # Predict ratings for all these items (limit for speed)\n",
    "        predictions = []\n",
    "        for item_id in items_to_recommend[:500]:  # Limit for speed\n",
    "            pred = algo.predict(user_id, item_id)\n",
    "            predictions.append((item_id, pred.est))\n",
    "        \n",
    "        # Sort by predicted rating and return top N\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [item for item, _ in predictions[:n]]\n",
    "    except Exception as e:\n",
    "        print(f\"Error for user {user_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get unique test users\n",
    "test_users = test_df['user_id'].unique()\n",
    "print(f\"Generating recommendations for {len(test_users)} test users...\")\n",
    "\n",
    "# Generate recommendations for each model\n",
    "user_recommendations = {}\n",
    "\n",
    "for i, user_id in enumerate(test_users):\n",
    "    if i % 100 == 0:\n",
    "        print(f\"Processed {i}/{len(test_users)} users...\")\n",
    "    \n",
    "    user_recommendations[user_id] = {\n",
    "        'knn': get_top_n_recommendations(knn_model, trainset, user_id, n=10),\n",
    "        'svd': get_top_n_recommendations(svd_model, trainset, user_id, n=10),\n",
    "        'top_pop': get_top_n_recommendations(top_pop_model, trainset, user_id, n=10)\n",
    "    }\n",
    "\n",
    "print(f\"Recommendations generated for all {len(test_users)} users!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating RMSE with fixed function...\n",
      "  TopPop: 6645 valid predictions, 0 errors\n",
      "  KNN: 6645 valid predictions, 0 errors\n",
      "  SVD: 6645 valid predictions, 0 errors\n",
      "\n",
      "------------------------------\n",
      "RMSE RESULTS\n",
      "------------------------------\n",
      "TopPop RMSE: 3.3354\n",
      "KNN RMSE: 0.9789\n",
      "SVD RMSE: 0.9333\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: RMSE Evaluation\n",
    "\n",
    "def calculate_rmse_fixed(model, test_tuples, model_name=\"\"):\n",
    "    \"\"\"Calculate RMSE with proper true ratings\"\"\"\n",
    "    predictions = []\n",
    "    errors = 0\n",
    "    \n",
    "    for user, item, true_rating in test_tuples:\n",
    "        try:\n",
    "            # Get prediction from model\n",
    "            pred = model.predict(user, item)\n",
    "            \n",
    "            if pred.est is not None and not np.isnan(pred.est):\n",
    "                predictions.append({\n",
    "                    'true': true_rating,\n",
    "                    'est': float(pred.est)\n",
    "                })\n",
    "            else:\n",
    "                errors += 1\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "    \n",
    "    print(f\"  {model_name}: {len(predictions)} valid predictions, {errors} errors\")\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        return float('nan')\n",
    "    \n",
    "    # Calculate RMSE manually\n",
    "    try:\n",
    "        squared_errors = [(p['true'] - p['est']) ** 2 for p in predictions]\n",
    "        mse = np.mean(squared_errors)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "    except Exception as e:\n",
    "        print(f\"  Error in manual RMSE calculation: {e}\")\n",
    "        return float('nan')\n",
    "\n",
    "# Calculate RMSE for each model\n",
    "print(\"\\nCalculating RMSE with fixed function...\")\n",
    "top_pop_rmse = calculate_rmse_fixed(top_pop_model, test_tuples, \"TopPop\")\n",
    "knn_rmse = calculate_rmse_fixed(knn_model, test_tuples, \"KNN\")\n",
    "svd_rmse = calculate_rmse_fixed(svd_model, test_tuples, \"SVD\")\n",
    "\n",
    "# Display RMSE results\n",
    "print(\"\\n\" + \"-\"*30)\n",
    "print(\"RMSE RESULTS\")\n",
    "print(\"-\"*30)\n",
    "print(f\"TopPop RMSE: {top_pop_rmse:.4f}\")\n",
    "print(f\"KNN RMSE: {knn_rmse:.4f}\")\n",
    "print(f\"SVD RMSE: {svd_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 6645 test interactions\n",
      "Found 1389 unique test users\n"
     ]
    }
   ],
   "source": [
    "# STEP 2: Prepare test data\n",
    "\n",
    "# Convert test data to list of tuples\n",
    "test_tuples = [tuple(x) for x in test_df[['user_id', 'item_id', 'rating']].to_numpy()]\n",
    "print(f\"Prepared {len(test_tuples)} test interactions\")\n",
    "\n",
    "# Get unique test users\n",
    "test_users = test_df['user_id'].unique()\n",
    "print(f\"Found {len(test_users)} unique test users\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recreating TopPop model...\n",
      "TopPop model recreated with 10 popular items\n",
      "Top items: ['B0086VPUHI', 'B00BN5T30E', 'B07YBXFDYN', 'B00BGA9WK2', 'B007CM0K86']...\n",
      "\n",
      "Models and data loaded successfully!\n",
      "Train set size: 26580\n",
      "Test set size: 6645\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Load saved models and data\n",
    "\n",
    "# Load the cleaned data\n",
    "train_df = pd.read_parquet('cleaned_train.parquet')\n",
    "test_df = pd.read_parquet('cleaned_test.parquet')\n",
    "\n",
    "# Load the saved models\n",
    "with open('best_knn_model.pkl', 'rb') as f:\n",
    "    knn_model = pickle.load(f)\n",
    "\n",
    "with open('best_svd_model.pkl', 'rb') as f:\n",
    "    svd_model = pickle.load(f)\n",
    "\n",
    "with open('trainset.pkl', 'rb') as f:\n",
    "    trainset = pickle.load(f)\n",
    "\n",
    "# RECREATE TopPop model\n",
    "print(\"\\nRecreating TopPop model...\")\n",
    "top_items = pd.read_csv('top_items.csv')\n",
    "top_pop_items = top_items.head(10)['item_id'].tolist()\n",
    "top_pop_model = TopPop(top_pop_items)\n",
    "top_pop_model.fit(trainset)\n",
    "\n",
    "print(f\"TopPop model recreated with {len(top_pop_items)} popular items\")\n",
    "print(f\"Top items: {top_pop_items[:5]}...\")\n",
    "\n",
    "print(\"\\nModels and data loaded successfully!\")\n",
    "print(f\"Train set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE THE TOPPOP CLASS\n",
    "\n",
    "class TopPop(AlgoBase):\n",
    "    def __init__(self, top_items_list=None):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.top_items = top_items_list if top_items_list else []\n",
    "    \n",
    "    def fit(self, trainset):\n",
    "        self.trainset = trainset\n",
    "        return self\n",
    "    \n",
    "    def estimate(self, u, i):\n",
    "        try:\n",
    "            raw_iid = self.trainset.to_raw_iid(i)\n",
    "            if raw_iid in self.top_items:\n",
    "                return 5.0\n",
    "            else:\n",
    "                return 1.0\n",
    "        except:\n",
    "            return 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from surprise import AlgoBase, accuracy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
